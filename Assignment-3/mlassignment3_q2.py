# -*- coding: utf-8 -*-
"""MLAssignment3Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13y3PvI8vhF09fUqfNoFq8zobsM-toJek
"""

def q2a():
  import pandas as pd
  import pickle
  from scipy.io import loadmat
  import numpy as np
  import matplotlib.pyplot as plt
  from matplotlib import cm
  mats = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/dataset_a.mat")
  unique_labels=np.unique(mats['labels'])
  unique_labels
  data=mats["samples"]
  labels=mats["labels"][0]
  x=np.zeros(10000)
  y=np.zeros(10000)
  for i in range(len(labels)):
    x[i]=data[i][0]
    y[i]=data[i][1]
  plt.figure(figsize=(10,6))
  scatter = plt.scatter(x,y,c=mats["labels"],cmap=plt.cm.get_cmap("prism_r",2),marker='o', s= 15)
  #plt.colorbar(ticks=range(2))
  plt.xlabel("samples_0th_index")
  plt.ylabel("samples_1st_index")
  plt.legend(handles=scatter.legend_elements()[0],labels=list(unique_labels))
  plt.title("scatter plot dor dataset_a.mat")
  plt.show()
q2a()

#from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np

def linear(X, Y):
        return np.dot(X, Y.T)

class SVM(object):
    """docstring for Regression."""
    def __init__(self):
        super(SVM, self).__init__()        
        self.support_vectors=None
        self.coef = None
        self.intercept = None
        self.duelcoef = None
    
    """You can give any required inputs to the fit()"""
    def fit(self, X_train, y_train, inpkernel, c):
        self.X_train = X_train
        self.y_train = y_train
        self.inpkernel = inpkernel
        self.c = c
        clr = SVC(kernel = self.inpkernel, C=self.c)
        #print(clr.kernel, clr.C)
        clr.fit(X_train, y_train)
        self.support_vectors=clr.support_vectors_
        self.coef = clr.coef_
        self.intercept = clr.intercept_
        self.duelcoef = clr.dual_coef_
        
        #print(self.intercept.shape, self.coef.shape)
        #Kernel
        m, n = X_train.shape  
        return self.support_vectors, self.coef, self.intercept

    """Here you can use the fit() from the LinearRegression of sklearn"""

    """ You can add as many methods according to your requirements, but training must be using fit(), and testing must be with predict()"""

    def predict(self,X_test):
        if(self.inpkernel == "linear"):
          linear_output=np.zeros((X_test.shape[0], 1))  
          linear_output = np.dot(X_test, self.coef.T) + self.intercept
          return np.sign(linear_output)
                  
        """ Write it from scratch usig oitcomes of fit()"""

        """Fill your code here. predict() should only take X_test and return predictions."""


def q2b():
    import numpy as np
    from sklearn.model_selection import train_test_split
    from scipy.io import loadmat

    mats = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/dataset_a.mat")
    unique_labels=np.unique(mats['labels'])
    #print(unique_labels)
    data=mats["samples"]
    labels=mats["labels"][0]
    X = data
    y = labels
    np.random.seed(1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)

    #visualise svm

    import numpy as np
    import matplotlib.pyplot as plt
    from mlxtend.plotting import plot_decision_regions
    from sklearn import metrics
    fignum = 1
    # # fit the model
    for penalty in (0.08, 0.15, 1):

        svm_obj = SVM()
        suppor_vectors, coef, intercept = svm_obj.fit(X_train, y_train, "linear", penalty)
        labels_predict_test = svm_obj.predict(X_test)
        #print(labels_predict_test)
        for j in range(X_test.shape[0]):
          if ((labels_predict_test[j]) == 1.):
            labels_predict_test[j] = 1
          else:
            labels_predict_test[j] = 0 
        accuracy = sum(y_test == labels_predict_test.flatten())/y_test.shape[0]
        print(f'Accuracy for c = ', penalty , ' is : ', accuracy)   
          
        # get the separating hyperplane
        w = coef[0]
        a = -w[0] / w[1]
        xx = np.linspace(-5, 5)
        yy = a * xx - (intercept[0]) / w[1]

        # plot the parallels to the separating hyperplane that pass through the
        # support vectors (margin away from hyperplane in direction
        # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in
        # 2-d.
        margin = 1 / np.sqrt(np.sum(coef ** 2))
        yy_down = yy - np.sqrt(1 + a ** 2) * margin
        yy_up = yy + np.sqrt(1 + a ** 2) * margin

        # plot the line, the points, and the nearest vectors to the plane
        plt.figure(fignum, figsize=(8,6))
        plt.clf()
        plt.plot(xx, yy, 'k-')
        plt.plot(xx, yy_down, 'k--')
        plt.plot(xx, yy_up, 'k--')

        plt.scatter(suppor_vectors[:, 0], suppor_vectors[:, 1], zorder=10, s=80,
                    facecolors='none', edgecolors='k', marker='o', color='red')
        # plt.scatter(suppor_vectors[:, 0], suppor_vectors[:, 1], s=20,
        #               marker='o', color='red')
        plt.scatter(X_train[:, 0], X_train[:, 1], zorder=10,  c=y_train, cmap=plt.cm.Paired,
                  edgecolors='k', s = 80)
        # plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, zorder=10, cmap='autumn',
        #             edgecolors='k')

        plt.axis('tight')
        x_min = -2.8
        x_max = 2.2
        y_min = -2
        y_max = 2

        XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
        Z = svm_obj.predict(np.c_[XX.ravel(), YY.ravel()])

        # Put the result into a color plot
        Z = Z.reshape(XX.shape)
        plt.figure(fignum, figsize=(8,6))
        plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)

        plt.xlim(x_min, x_max)
        plt.ylim(y_min, y_max)

        # plt.xticks(())
        # plt.yticks(())
        fignum = fignum + 1
        plt.title("c : "+ str(penalty))
    plt.show()
q2b()

#from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np

class SVM2(object):
    def __init__(self,c,g):
        super(SVM2, self).__init__()   
        self.clr = SVC(kernel='rbf',C=c,gamma=g)     
        self.support_vectors=None
        self.coef = None
        self.intercept = None
        self.duelcoef = None
    
    """You can give any required inputs to the fit()"""
    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train
        #self.inpkernel = inpkernel
        #self.c = c
        #print(clr.kernel, clr.C)
        self.clr.fit(X_train, y_train)
        self.support_vectors=self.clr.support_vectors_
        #self.coef = self.clr.coef_
        self.intercept = self.clr.intercept_
        self.duelcoef = self.clr.dual_coef_
        
        #print(self.intercept.shape, self.coef.shape)
        #Kernel
        m, n = X_train.shape  
        return self.support_vectors, self.coef, self.intercept

    def predict(self,X_test):
        y_pred = []
        val = self.clr.decision_function(X_test)
        for v in range(len(val)):
          if val[v]>0:
            y_pred.append(1)
          else:
            y_pred.append(0)
        y_pred = np.array(y_pred).reshape(-1)
        return y_pred

    def decision_function(self,dd):
        res = self.clr.decision_function(dd)
        return res

def q2c():
    import numpy as np
    from sklearn.model_selection import train_test_split
    from scipy.io import loadmat

    mats = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/dataset_a.mat")
    unique_labels=np.unique(mats['labels'])
    #print(unique_labels)
    data=mats["samples"]
    labels=mats["labels"][0]
    X = data
    y = labels
    np.random.seed(1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)

    #visualise svm

    import numpy as np
    import matplotlib.pyplot as plt
    import numpy as np
    from sklearn import svm
    from sklearn import metrics
    fignum = 1
    # # fit the model
    for [c,g] in [[0.08,0.1],[0.1,0.1],[0.15,0.2],[0.3,0.4],[0.5,0.6],[1,0.8]]:
        svm_obj = SVM2(c,g)
        suppor_vectors, coef, intercept = svm_obj.fit(X_train, y_train)
        labels_predict_test = svm_obj.predict(X_test)
        #print(labels_predict_test)
        for j in range(X_test.shape[0]):
          if ((labels_predict_test[j]) == 1.):
            labels_predict_test[j] = 1
          else:
            labels_predict_test[j] = 0 
        accuracy = sum(y_test == labels_predict_test.flatten())/y_test.shape[0]
        print('Accuracy for c = ', c ,'and for gamma = ', g , ' is : ', accuracy)   

        plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)

        # plot the decision function
        ax = plt.gca()
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()

        # create grid to evaluate model
        xx = np.linspace(xlim[0], xlim[1], 30)
        yy = np.linspace(ylim[0], ylim[1], 30)
        YY, XX = np.meshgrid(yy, xx)
        xy = np.vstack([XX.ravel(), YY.ravel()]).T
        Z = svm_obj.decision_function(xy).reshape(XX.shape)

        # plot decision boundary and margins
        ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
           linestyles=['--', '-', '--'])
        # plot support vectors
        ax.scatter(svm_obj.support_vectors[:, 0], svm_obj.support_vectors[:, 1], s=100,
           linewidth=1, facecolors='none', edgecolors='k')
        plt.show()  

q2c()

def q2d():
    import numpy as np
    from sklearn.model_selection import train_test_split
    from scipy.io import loadmat

    mats = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/dataset_a.mat")
    unique_labels=np.unique(mats['labels'])
    #print(unique_labels)
    data=mats["samples"]
    labels=mats["labels"][0]
    X = data
    y = labels
    np.random.seed(1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)

    #visualise svm

    import numpy as np
    #import matplotlib.pyplot as plt
    from sklearn.svm import SVC
    import numpy as np
    from sklearn import svm
    from sklearn.metrics import accuracy_score
    # # fit the model
    cls = SVC(kernel = 'linear', C = 0.15)
    cls.fit(X_train,y_train)
    y_pred = cls.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Accuracy in Linear SVM for optimum C value 0.15 is ',accuracy)

    clr = SVC(kernel = 'rbf', C = 0.15, gamma = 0.1)
    clr.fit(X_train,y_train)
    y_pred = clr.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Accuracy in SVM with RBF kernel for optimum C value 0.15 and optimal Gamma value 0.1 is ',accuracy) 
q2d()

from google.colab import drive
drive.mount('/content/drive')