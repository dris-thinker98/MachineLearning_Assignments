# -*- coding: utf-8 -*-
"""MLAssignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vpZloQvPQrMyh2Mh91uL2w61YoBDRgqU

**Answer 1a**
"""

# Commented out IPython magic to ensure Python compatibility.
def q1a():
    import numpy as np # Import relevant libraries
    import cv2
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    import tarfile, pickle
    from scipy.io import loadmat
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import scale
    import plotly.express as px
    from sklearn.manifold import TSNE
#     %matplotlib inline

    df1 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_1.mat")
    df2 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_2.mat")
    df3 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_3.mat")
    df4 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_4.mat")
    df5 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_5.mat")
    df_test = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/test_batch.mat")

    dfcom1 = np.append(df1["data"], df2["data"], axis=0)
    dfcom2 = np.append(df3["data"], df4["data"], axis=0)
    dfcom3 = np.append(dfcom1, dfcom2, axis = 0)
    df_data_train = np.append(dfcom3, df5["data"], axis =0)
    dflabelcom1 = np.append(df1["labels"], df2["labels"], axis =0)
    dflabelcom2 = np.append(df3["labels"], df4["labels"], axis =0)
    dflabelcom3 = np.append(dflabelcom1, dflabelcom2, axis =0)
    df_labels_train = np.append(dflabelcom3, df5["labels"], axis = 0)
    print(df_data_train.shape)
    print(df_labels_train.shape)
    x_test = df_test["data"]
    y_test = df_test["labels"]

    # retain_variance_pca = PCA(.90)
    # p_components = retain_variance_pca.fit_transform(X = df_data_train)

    # print(retain_variance_pca.n_components_)

    print(df_data_train.shape)

    # color histogram from scratch
    # commented to load data from saved file
    # images = np.reshape(df_data_train, (50000, 3, 32, 32))
    # print(images.shape)
    # imagesarray_new = np.zeros((50000, 32, 32, 3))
    # imagesarray_new = np.array(imagesarray_new, dtype=np.uint8)
    # huelist = list()
    # hist_values = np.zeros((50000, 180))
    # for i in range(df_data_train.shape[0]):
    #   single_img = images[i]
    #   #imagesarray_new[i] = single_img.reshape(32,32,3)
    #   imagesarray_new[i] = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))
    #   #imagesarray_new = np.array(imagesarray_new, dtype=np.uint8)
    #   imagesarray_new[i] = cv2.cvtColor(imagesarray_new[i], cv2.COLOR_RGB2BGR)
    #   imagesarray_new[i] = cv2.cvtColor(imagesarray_new[i],cv2.COLOR_BGR2HSV)
    #   hue ,  sat ,  val  =  imagesarray_new[i][:,:, 0 ],  imagesarray_new[i][:,:, 1 ],  imagesarray_new[i][:,:, 2 ]
    #   huelist = np.ndarray.flatten(hue) # bins = 180 
    #   # plt.hist(np.ndarray.flatten(hue), bins=180)
    #   for bin in range(180):
    #     hist_values[i][bin] = huelist.tolist().count(bin)

    #print(imagesarray_new.shape)

    import numpy as np
    import matplotlib.pyplot as plt
    from skimage.io import imread, imshow
    import skimage.color
    from skimage.transform import resize
    from skimage.feature import hog
    import pandas as pd

    #hog from scratch

    def c_grad(img, template):
      ts = template.size #Number of elements in the template.

      #New padded array to hold the resultant gradient image.
      new_img = np.zeros((img.shape[0]+ts-1, img.shape[1]+ts-1))
      new_img[1:img.shape[0] + 1, 1:img.shape[1]+1] = img
      result = np.zeros((new_img.shape))

      for r in range(1, img.shape[0] + 1):
          for c in range(1, img.shape[1] + 1):
                                
              curr_region = new_img[r-1:r+1+1, 
                                    c-1:c+1+1]
              curr_result = curr_region * template
              score = np.sum(curr_result)
              result[r, c] = score
      #Result of the same size as the original image after removing the padding.
      result_img = result[1:result.shape[0]-1, 
                          1:result.shape[1]-1]
      return result_img

    def grad_direc(horz_grad, vert_grad):
      grad_direction = np.arctan(vert_grad/(horz_grad+0.00000001))
      grad_direction = np.rad2deg(grad_direction)
      grad_direction = grad_direction + 90
      return grad_direction

    def grad_mag(horz_grad, vert_grad):

      sum_squares = np.power(horz_grad, 2) + np.power(vert_grad, 2)
      grad_magnitude = np.sqrt(sum_squares)
      return grad_magnitude

    def hog_cell_histgm(cell_direction, cell_magnitude, hist_bins):
        HOG_cell_hist = np.zeros(shape=(hist_bins.size))
        cell_size = cell_direction.shape[0]

        for row_idx in range(cell_size):
            for col_idx in range(cell_size):
                curr_direction = cell_direction[row_idx, col_idx]
                curr_magnitude = cell_magnitude[row_idx, col_idx]

                diff = np.abs(curr_direction - hist_bins)

                if curr_direction < hist_bins[0]:
                    first_bin_idx = 0
                    second_bin_idx = hist_bins.size-1
                elif curr_direction > hist_bins[-1]:
                    first_bin_idx = hist_bins.size-1
                    second_bin_idx = 0
                else:
                    first_bin_idx = np.where(diff == np.min(diff))[0][0]
                    temp = hist_bins[[(first_bin_idx-1)%hist_bins.size, (first_bin_idx+1)%hist_bins.size]]
                    temp2 = np.abs(curr_direction - temp)
                    res = np.where(temp2 == np.min(temp2))[0][0]
                    if res == 0 and first_bin_idx != 0:
                        second_bin_idx = first_bin_idx-1
                    else:
                        second_bin_idx = first_bin_idx+1

                first_bin_value = hist_bins[first_bin_idx]
                second_bin_value = hist_bins[second_bin_idx]
                HOG_cell_hist[first_bin_idx] = HOG_cell_hist[first_bin_idx] + (np.abs(curr_direction - first_bin_value)/(180.0/hist_bins.size)) * curr_magnitude
                HOG_cell_hist[second_bin_idx] = HOG_cell_hist[second_bin_idx] + (np.abs(curr_direction - second_bin_value)/(180.0/hist_bins.size)) * curr_magnitude
        return HOG_cell_hist


    # hog_features = pd.DataFrame() 
    # for image in images:
    #   hog_per_cell = []
    #   a = 0
    #   b = 0
    #   c = 8
    #   d = 8
    #   # single_img = images[i]
    #   #imagesarray_new[i] = np.transpose(np.reshape(image,(3, 32,32)), (1,2,0))
    #   #img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    #   img = skimage.color.rgb2gray(image)
    #   horizontal_mask = np.array([-1, 0, 1])
    #   vertical_mask = np.array([[-1], [0], [1]])

    #   horizontal_grad = c_grad(img, horizontal_mask) 
    #   vertical_grad = c_grad(img, vertical_mask)

    #   grad_mag_array = grad_mag(horizontal_grad, vertical_grad)
    #   grad_direc_array = grad_direc(horizontal_grad, vertical_grad)

    #   grad_direc_array = grad_direc_array +90
    #   hist_bins = np.array([20,40,60,80,100,120,140,160,180])
    #   
    #   # Histogram of the first cell in the first block.
    #   for i in range(4):
    #     b = 0
    #     d = 8
    #     for j in range(4):
          
    #       cell_direc = grad_direc_array[a:c, b:d]
    #       cell_mag = grad_mag_array[a:c, b:d]
    #       per_cell_value = hog_cell_histgm(cell_direc, cell_mag, hist_bins)
    #       hog_per_cell.extend(per_cell_value)
    #       b += 8
    #       d += 8
    #     a += 8
    #     c += 8
    #   hog_feature_per_sample = np.array(hog_per_cell)
    #   hog_features = hog_features.append(pd.Series(hog_feature_per_sample), ignore_index = True)
    # np.save("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_hog.npy",hog_features)
    hog_features=np.load("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_hog.npy")
    #print(hist_values.shape, hog_features.shape)
    # hog_hist_features_combined = np.concatenate((hist_values, hog_features), axis=1)
    # hog_hist_features_combined.shape
    # np.save("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_hog+histcombined.npy",hog_hist_features_combined)
    hog_hist_features_combined=np.load("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_hog+histcombined.npy")

q1a()

"""**Answer 1b**"""

# Commented out IPython magic to ensure Python compatibility.
def q1b():

    import numpy as np
    import matplotlib.pyplot as plt

    print(np.unique(df_labels_train))
    # optional 
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    import tarfile, pickle
    from scipy.io import loadmat
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import scale
    import plotly.express as px
    from sklearn.manifold import TSNE
#     %matplotlib inline

    df1 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_1.mat")
    df2 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_2.mat")
    df3 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_3.mat")
    df4 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_4.mat")
    df5 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_5.mat")
    df_test = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/test_batch.mat")

    dfcom1 = np.append(df1["data"], df2["data"], axis=0)
    dfcom2 = np.append(df3["data"], df4["data"], axis=0)
    dfcom3 = np.append(dfcom1, dfcom2, axis = 0)
    df_data_train = np.append(dfcom3, df5["data"], axis =0)
    dflabelcom1 = np.append(df1["labels"], df2["labels"], axis =0)
    dflabelcom2 = np.append(df3["labels"], df4["labels"], axis =0)
    dflabelcom3 = np.append(dflabelcom1, dflabelcom2, axis =0)
    df_labels_train = np.append(dflabelcom3, df5["labels"], axis = 0)
    print(df_data_train.shape)
    print(df_labels_train.shape)
    x_test = df_test["data"]
    y_test = df_test["labels"]

    # commented as number of components calculated
    # pca2 = PCA().fit(df_data_train)
    # plt.plot(np.cumsum(pca2.explained_variance_ratio_))
    # plt.xlabel('number of components')
    # plt.ylabel('cumulative explained variance');
    # pca1 = PCA(0.90).fit(df_data_train)
    # print(pca1.n_components_)

    # commented to load input from file
    pca = PCA(n_components=99)
    # pca.fit(df_data_train)
    # X1=pca.fit_transform(df_data_train)
    x1_test = pca.fit_transform(x_test)
    # np.save("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_pca.npy",X1)
    X1=np.load("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_pca.npy")
    print(X1)
    #commented to check the loaded file
    # tsne=TSNE(n_components=2, random_state=0)
    # tsne_results = tsne.fit_transform(X1)
    # np.save("/content/drive/My Drive/Colab Notebooks/MLAssignment3/tsne_saved_pca.npy",tsne_results)
    tsne_results=np.load("/content/drive/My Drive/Colab Notebooks/MLAssignment3/tsne_saved_pca.npy")
    x=np.zeros(50000)
    y=np.zeros(50000)
    plt.figure(figsize=(12,8))
    x=tsne_results[:,0]
    y=tsne_results[:,1]

    # using maltplotlib scatter
    # scatter = plt.scatter(x,y,c=df_labels_train[:,0],cmap=plt.cm.get_cmap("prism", 10),marker="o", s = 10)
    # plt.legend(handles = scatter.legend_elements()[0],labels=list(np.unique(df_labels_train)), loc='best')
    # #plt.colorbar(ticks=range(10))
    # plt.xlabel("TSNE_1st_feature")
    # plt.ylabel("TSNE_2nd_feature")
    # plt.title("Tsne for pca")
    # plt.show()

    import seaborn as sns
    plt.figure(figsize=(16,8))
    sns.scatterplot(x = x, y = y, hue = df_labels_train[:,0], palette = sns.color_palette("hls", 10), data = X1, legend = "full",)
    plt.xlabel("TSNE_1st_feature")
    plt.ylabel("TSNE_2nd_feature")
    plt.title("Tsne for pca")
    plt.show()


    #tsne for hog+hist

    # tsne2 = TSNE(n_components=2, random_state=0)
    # tsne_results2 = tsne2.fit_transform(hog_hist_features_combined)
    # np.save("/content/drive/My Drive/Colab Notebooks/MLAssignment3/tsne_saved_hog+hist.npy",tsne_results2)
    tsne_results2 = np.load("/content/drive/My Drive/Colab Notebooks/MLAssignment3/tsne_saved_hog+hist.npy")
    x2=np.zeros(50000)
    y2=np.zeros(50000)
    plt.figure(figsize=(12,8))
    x2=tsne_results2[:,0]
    y2=tsne_results2[:,1]

    plt.figure(figsize=(16,8))
    sns.scatterplot(x = x2, y = y2, hue = df_labels_train[:,0], palette = sns.color_palette("hls", 10), data = hog_hist_features_combined, legend = "full",)
    plt.xlabel("TSNE_1st_feature")
    plt.ylabel("TSNE_2nd_feature")
    plt.title("Tsne for hog + hist")
    plt.show()

q1b()

"""**Answer 1c**"""

# Commented out IPython magic to ensure Python compatibility.
def q1c1():
    from sklearn.model_selection import GridSearchCV
    from sklearn import svm
    from sklearn.svm import SVC
    import time 
    import pickle

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    import tarfile, pickle
    from scipy.io import loadmat
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import scale
    import plotly.express as px
    from sklearn.manifold import TSNE
#     %matplotlib inline

    df1 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_1.mat")
    df2 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_2.mat")
    df3 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_3.mat")
    df4 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_4.mat")
    df5 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_5.mat")
    df_test = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/test_batch.mat")

    dfcom1 = np.append(df1["data"], df2["data"], axis=0)
    dfcom2 = np.append(df3["data"], df4["data"], axis=0)
    dfcom3 = np.append(dfcom1, dfcom2, axis = 0)
    df_data_train = np.append(dfcom3, df5["data"], axis =0)
    dflabelcom1 = np.append(df1["labels"], df2["labels"], axis =0)
    dflabelcom2 = np.append(df3["labels"], df4["labels"], axis =0)
    dflabelcom3 = np.append(dflabelcom1, dflabelcom2, axis =0)
    df_labels_train = np.append(dflabelcom3, df5["labels"], axis = 0)
    print(df_data_train.shape)
    print(df_labels_train.shape)
    x_test = df_test["data"]
    y_test = df_test["labels"]


    parameters = { 'C':[1, 10]}
    svc = SVC(kernel='rbf')
    scaler = MinMaxScaler().fit(X1)
    pca = PCA(n_components=99)
    # pca.fit(df_data_train)
    # X1=pca.fit_transform(df_data_train)
    x1_test = pca.fit_transform(x_test)
    X1 = scaler.transform(X1)
    #commented to check the loaded file
    # clf = GridSearchCV(svc, parameters, n_jobs = -1, verbose = 3, refit = True, cv = 5)
    # clf.fit(X1, df_labels_train[:, 0])
    # pickle.dump(clf, open("/content/drive/My Drive/Colab Notebooks/MLAssignment3/gridsearch_pca.pkl", 'wb' ))
    clf_from_pickle = pickle.load(open("/content/drive/My Drive/Colab Notebooks/MLAssignment3/gridsearch_pca.pkl", 'rb'))

    #clf_from_pickle.support_
    print(clf_from_pickle.cv_results_)
    support_vector_indices = (clf_from_pickle.best_estimator_.support_)
    print(clf_from_pickle.best_params_)
    print(clf_from_pickle.best_score_)
    print(clf_from_pickle.best_estimator_)
    y_pred2_train = clf_from_pickle.predict(x1_test)

q1c1()
# sorted(clf.cv_results_.keys())
# help(type(self))

"""**Answer 1c for hog+hist**"""

# Commented out IPython magic to ensure Python compatibility.
def q1c2():
    # Gridsearch for hog+hist features
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import GridSearchCV
    from sklearn import svm
    from sklearn.svm import SVC
    import time 
    import pickle
    import numpy as np

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    import tarfile, pickle
    from scipy.io import loadmat
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import scale
    import plotly.express as px
    from sklearn.manifold import TSNE
#     %matplotlib inline

    df1 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_1.mat")
    df2 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_2.mat")
    df3 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_3.mat")
    df4 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_4.mat")
    df5 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_5.mat")
    df_test = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/test_batch.mat")

    dfcom1 = np.append(df1["data"], df2["data"], axis=0)
    dfcom2 = np.append(df3["data"], df4["data"], axis=0)
    dfcom3 = np.append(dfcom1, dfcom2, axis = 0)
    df_data_train = np.append(dfcom3, df5["data"], axis =0)
    dflabelcom1 = np.append(df1["labels"], df2["labels"], axis =0)
    dflabelcom2 = np.append(df3["labels"], df4["labels"], axis =0)
    dflabelcom3 = np.append(dflabelcom1, dflabelcom2, axis =0)
    df_labels_train = np.append(dflabelcom3, df5["labels"], axis = 0)
    print(df_data_train.shape)
    print(df_labels_train.shape)
    x_test = df_test["data"]
    y_test = df_test["labels"]

    parameters2 = { 'C':[ 10]}
    svc2 = SVC(kernel='rbf')


    hog_hist_features_combined = np.load("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_hog+histcombined.npy")
    hog_hist_features_combined_test=np.load("/content/drive/My Drive/Colab Notebooks/MLAssignment3/saved_hog+histcombined_test.npy")
    scaler = MinMaxScaler()
    scaler2 =  MinMaxScaler()
    scaler.fit(hog_hist_features_combined)
    hog_hist_features_combined = scaler.transform(hog_hist_features_combined)
    scaler2.fit(hog_hist_features_combined_test)
    hog_hist_features_combined_test = scaler2.transform(hog_hist_features_combined_test)
    clf2 = GridSearchCV(svc2, parameters2, n_jobs = -1, verbose = 3, refit = True, cv = 5)
    clf2.fit(hog_hist_features_combined, df_labels_train[:, 0])
    pickle.dump(clf2, open("/content/drive/My Drive/Colab Notebooks/MLAssignment3/gridsearch_hog+hist.pkl", 'wb' ))
    clf_from_pickle2 = pickle.load(open("/content/drive/My Drive/Colab Notebooks/MLAssignment3/gridsearch_hog+hist.pkl", 'rb'))

    #clf_from_pickle.support_
    print(clf_from_pickle2.cv_results_)
    support_vector_indices2 = (clf_from_pickle2.best_estimator_.support_)
    print(clf_from_pickle2.best_params_)
    print(clf_from_pickle2.best_score_)
q1c2()

"""**Answer 1d**"""

# Commented out IPython magic to ensure Python compatibility.
def q1d():
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    import tarfile, pickle
    from scipy.io import loadmat
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import scale
    import plotly.express as px
    from sklearn.manifold import TSNE
#     %matplotlib inline

    df1 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_1.mat")
    df2 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_2.mat")
    df3 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_3.mat")
    df4 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_4.mat")
    df5 = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/data_batch_5.mat")
    df_test = loadmat("/content/drive/My Drive/Colab Notebooks/MLAssignment3/test_batch.mat")

    dfcom1 = np.append(df1["data"], df2["data"], axis=0)
    dfcom2 = np.append(df3["data"], df4["data"], axis=0)
    dfcom3 = np.append(dfcom1, dfcom2, axis = 0)
    df_data_train = np.append(dfcom3, df5["data"], axis =0)
    dflabelcom1 = np.append(df1["labels"], df2["labels"], axis =0)
    dflabelcom2 = np.append(df3["labels"], df4["labels"], axis =0)
    dflabelcom3 = np.append(dflabelcom1, dflabelcom2, axis =0)
    df_labels_train = np.append(dflabelcom3, df5["labels"], axis = 0)
    print(df_data_train.shape)
    print(df_labels_train.shape)
    x_test = df_test["data"]
    y_test = df_test["labels"]


    #new training data
    clf_from_pickle = pickle.load(open("/content/drive/My Drive/Colab Notebooks/MLAssignment3/gridsearch_pca.pkl", 'rb'))

    #clf_from_pickle.support_
    support_vector_indices = (clf_from_pickle.best_estimator_.support_)

    # indices = model.support_
    x_train_new_pca = clf_from_pickle.best_estimator_.support_vectors_
    print(x_train_new_pca.shape)

    y_train = df_labels_train[:,0]
    y_train_new_pca = y_train[support_vector_indices]
    print(y_train_new_pca.shape)

    #Import svm model
    from sklearn import svm

    #Create a svm Classifier
    clf3 = svm.SVC(kernel='rbf') # Linear Kernel

    #Train the model using the training sets
    clf3.fit(x_train_new_pca, y_train_new_pca)

    #Predict the response for test dataset
    y_pred3 = clf3.predict(x1_test)
    from sklearn import metrics
    print(x1_test.shape)
    print(y_pred3.shape)
    # Model Accuracy: how often is the classifier correct?
    y_pred3_train = clf3.predict(x_train_new_pca)
    print("Test Accuracy:",metrics.accuracy_score(y_test.flatten(), y_pred3))
    print("Train Accuracy:",metrics.accuracy_score(y_train_new_pca, y_pred3_train))


    clf_from_pickle2 = pickle.load(open("/content/drive/My Drive/Colab Notebooks/MLAssignment3/gridsearch_hog+hist.pkl", 'rb'))
    #clf_from_pickle.support_
    support_vector_indices2 = (clf_from_pickle2.best_estimator_.support_)

    # indices = model.support_
    x_train_new_pca2 = clf_from_pickle2.best_estimator_.support_vectors_
    print(x_train_new_pca2.shape)

    y_train = df_labels_train[:,0]
    y_train_new_pca2 = y_train[support_vector_indices2]
    print(y_train_new_pca2.shape)

    #Import svm model
    from sklearn import svm

    #Create a svm Classifier
    clf4 = svm.SVC(kernel='rbf') # Linear Kernel

    #Train the model using the training sets
    clf4.fit(x_train_new_pca2, y_train_new_pca2)

    #Predict the response for test dataset
    y_pred4 = clf4.predict(hog_hist_features_combined_test)
    from sklearn import metrics
    print(hog_hist_features_combined_test.shape)
    print(y_pred4.shape)
    # Model Accuracy: how often is the classifier correct?
    y_pred4_train = clf4.predict(x_train_new_pca2)
    print("Test Accuracy:",metrics.accuracy_score(hog_hist_features_combined_test.flatten(), y_pred4))
    print("Train Accuracy:",metrics.accuracy_score(y_train_new_pca2, y_pred4_train))
q1d()