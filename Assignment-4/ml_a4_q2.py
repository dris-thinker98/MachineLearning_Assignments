# -*- coding: utf-8 -*-
"""MLAssignment4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fuIcF8pfLkN9fVfrSKp3C4n8tgy-RLmw
"""

def q2():
  #data manipulation
  import pickle
  #methods and stopwords text processing
  from sklearn.model_selection import train_test_split
  from sklearn.model_selection import StratifiedShuffleSplit
  from nltk.corpus import stopwords
  from nltk.tokenize import word_tokenize
  from sklearn.model_selection import train_test_split
  from collections import Counter

  #machine learning libraries
  import nltk 
  from sklearn.metrics import accuracy_score
  import pandas as pd
  import numpy as np
  import string 

  #q2 - part(1)
  dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MLAssignment4/yelp_labelled.txt',  delimiter="\t",  header = None)
  dataset.columns = ['text','target']

  # dataset=pd.read_csv("filepath.txt",delimiter="\t")
  datasetpd = dataset.to_csv('/content/drive/My Drive/Colab Notebooks/MLAssignment4/yelp_labelled.csv', index = None)
  print(dataset)

  X = dataset['text']
  y = dataset['target']

  x_train, x_test, y_train, y_test = train_test_split(X, y,
                                                      test_size=0.30,
                                                      random_state=0,
                                                      stratify = y)
  x_traincopy = x_train
  x_testcopy = x_test
  # sss = StratifiedShuffleSplit(test_size=0.3, random_state=0)
  # sss.get_n_splits(X, y)
  print(f'Data Split done.')

  # for train_index, test_index in sss.split(X, y):
  #   #print("TRAIN:", train_index, "TEST:", test_index)
  #   X_train, X_test = X[train_index], X[test_index]
  #   y_train, y_test = y[train_index], y[test_index]


  #q2 - part(2)

  def preprocess(xtrain):
    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('wordnet')
    stop_words = set(stopwords.words('english'))
    processedText = []
    for text in xtrain:
      #remove punctuations
      text_wp = text.translate(str.maketrans("", "", string.punctuation))
      #Lowercasing all words
      text_lc = text_wp.lower()
      #removing stopwords
      text_tokens = word_tokenize(text_lc)
      filtered_text = [ word for word in text_tokens if word not in stop_words]

      processedText.append(filtered_text)

      # word_tokens = word_tokenize(text) 
    
      # # filtered_sentence = [w for w in word_tokens if not w in stop_words] 
        
      # filtered_sentence = [] 
        
      # for w in word_tokens: 
      #     if w not in stop_words: 
      #         filtered_sentence.append(w) 
        
      # print(word_tokens) 
      # print(filtered_sentence)
          
    return processedText
  #q2 - part(3)

  x = preprocess(x_train)
  xtest_preprocessed = preprocess(x_test)
  #len(x)
  

  all_words = []
  for i in range(len(x)):
    #word_string = ' '.join(x[i])
    #all_words.append(word_string)
    all_words = all_words + x[i]
  unique_word_dictionary = set(all_words) 

  #q2 - part(4)

  def word_doc_matrix(x, unique_word_dictionary):
    word_count_all =[]
    rows, cols = (len(x), len(unique_word_dictionary))
    unique_word_count = [[0 for i in range(cols)] for j in range(rows)] 
    xnew = np.empty(rows)
    for j in range(len(x)):
      word_count = Counter(x[j])
      word_count_all.append(word_count)
      k = 0
      
      for word in unique_word_dictionary:
        unique_word_count[j][k] = x[j].count(word)
        k+=1
    return unique_word_count
      # xnew[j] = np.array(x[j])
      # count_row = np.bincount(xnew[j])
  final_word_doc_matrix = word_doc_matrix(x, unique_word_dictionary)
  final_word_doc_matrix_test = word_doc_matrix(xtest_preprocessed, unique_word_dictionary)
  from sklearn.naive_bayes import MultinomialNB
  clf = MultinomialNB(alpha=1.0,)
  clf.fit(np.array(final_word_doc_matrix), y_train)
  final_word_doc_matrix_test = (np.array(final_word_doc_matrix_test, dtype=object))
  y_pred_test = (clf.predict(final_word_doc_matrix_test))
  y_pred_train = clf.predict(np.array(final_word_doc_matrix))

  #q2 - part(5)

  test_accuracy = clf.score(final_word_doc_matrix_test, y_test)
  train_accuracy = clf.score(np.array(final_word_doc_matrix),y_train)
  print("\nTest_accuracy : ", test_accuracy, "\nTrain_accuracy : ", train_accuracy)
  y_test = y_test.to_numpy()
  y_train = y_train.to_numpy()
  counter = 0
  counter2 = 0
  for k in range(y_test.shape[0]):
    if(y_pred_test[k] != y_test[k]):
      print("\nInstance number in test_set : ", k, "  Predicted value : ", y_pred_test[k], "  Actual value : ", y_test[k], "original sentence : ", x_testcopy.iloc[k], '\n')
      counter += 1
  print("\nTotal misclassifications : ", counter)
  for m in range(y_train.shape[0]):
    if(y_pred_train[m] != y_train[m]):
      print("\nInstance number in train_set : ", m, "  Predicted value : ", y_pred_train[m], "  Actual value : ", y_train[m], "original sentence : ", x_traincopy.iloc[m] ,'\n')
      counter2 += 1
  print("\nTotal misclassifications : ", counter2)
  # print(x)
  # print(all_words)
  # print(len(all_words))
  # print(unique_word_dictionary)
  # print(len(unique_word_dictionary))
  # print(word_count)
q2()

