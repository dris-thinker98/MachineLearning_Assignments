# -*- coding: utf-8 -*-
"""MLAssignment6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OfaYuaRUFmtwnpOSFBCVNNjAFW_-nP_W
"""

# Commented out IPython magic to ensure Python compatibility.
def q2():
  #data manipulation
  import pickle
  #methods and stopwords text processing
  from nltk.corpus import stopwords
  from nltk.tokenize import word_tokenize
  from sklearn.model_selection import train_test_split
  from collections import Counter
  from tensorflow.keras.preprocessing.text import Tokenizer
  from tensorflow.keras.preprocessing.sequence import pad_sequences
  from tensorflow.keras.models import Sequential, load_model
  from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout, SimpleRNN
  from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D 
  from tensorflow.keras.optimizers import Adam
  import matplotlib.pyplot as plt
  import tensorflow as tf
  import pickle
  #machine learning libraries
  import nltk 
  from sklearn.metrics import accuracy_score
  import pandas as pd
  import numpy as np
  import string

  #q2 - part(1)
  dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/MLAssignment6/ML_A6_Q2_data.txt',  delimiter="\t",  header= None, quoting= 3, error_bad_lines= True, names= ['Sentence','Tag'], usecols=['Sentence','Tag'] )[['Sentence','Tag']]
  #dataset.columns = ['Sentence','Tag']
  dataset.to_csv('/content/drive/MyDrive/Colab Notebooks/MLAssignment6/ML_A6_Q2_data.csv', index = None)
  display(dataset)
  
  X = dataset['Sentence']
  y = dataset['Tag']

  def preprocess(xtrain):
    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('wordnet')
    stop_words = set(stopwords.words('english'))
    processedText = []
    sentence_text = []
    for text in xtrain:
      #remove punctuations
      text_wp = text.translate(str.maketrans("", "", string.punctuation))
      #Lowercasing all words
      text_lc = text_wp.lower()
      
      #removing stopwords
      text_tokens = word_tokenize(text_lc)
      filtered_text = [ word for word in text_tokens if word not in stop_words]
      processedText.append(filtered_text)

      text_final = ' '.join([t for t in text_lc.split() if t not in stop_words])
      sentence_text.append(text_final)

    return processedText,  sentence_text

  X_preprocessed, text_l = preprocess(X)
  # print(X_preprocessed)
  # print(text_l)
  
  token = Tokenizer()
  token.fit_on_texts(text_l)
  vocab_size  = len(token.word_index) + 1
  encoded_text = token.texts_to_sequences(text_l)
  # print(encoded_text)
  max_length = 100
  encoded_text_padded = pad_sequences(encoded_text, maxlen=max_length, padding='post')
  # print(encoded_text_padded.shape)

  x_train, x_test, y_train, y_test = train_test_split(encoded_text_padded, y,
                                                      test_size=0.30,
                                                      random_state=0)
  
#   %%time
  glove_vectors = dict()
  file = open('/content/drive/MyDrive/Colab Notebooks/MLAssignment6/glove.6B.50d.txt', encoding='utf-8')

  for line in file:
      values = line.split()
      word = values[0]
      #storing the word in the variable
      vectors = np.asarray(values[1: ])
      #storing the vector representation of the respective word in the dictionary
      glove_vectors[word] = vectors
  file.close()
  # print(len(glove_vectors))
  # print(glove_vectors.get('lost').shape)

  word_vector_matrix = np.zeros((vocab_size, 50))
  words_not_present = []
  for word, index in token.word_index.items():
    vector = glove_vectors.get(word)
    if vector is not None:
        word_vector_matrix[index] = vector
    else:
      words_not_present.append(word)
      # print(word)

  vec_size = 50
  embedding_layer = Embedding(vocab_size, vec_size, weights = [word_vector_matrix], input_length = max_length, trainable = False)
  # embedding_layer = Embedding(vocab_size, vec_size, weights = [word_vector_matrix], trainable = False)
  model = Sequential()
  model.add(embedding_layer)
  # model.add(SimpleRNN(80, return_sequences = True))
  model.add(SimpleRNN(80))
  # Add a Dense layer with 1 unit.
  model.add(Dense(1, activation = 'sigmoid'))
  model.summary()
  
  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])
  
  # Training the model
  history = model.fit(x_train, y_train, epochs=30, verbose=1, validation_data =(x_test, y_test))
  plt.figure(figsize=(8,6))
  scatter1 = plt.plot(history.history['accuracy'] , label="Training accuracy")
  scatter1 = plt.plot(history.history['val_accuracy'], label="Test accuracy")
  plt.xlabel('epochs')
  plt.ylabel('accuracy')
  plt.legend(loc='best')
  plt.show()
  plt.figure(figsize=(8,6))
  scatter2 = plt.plot(history.history['loss'], label="Training loss")
  scatter2 = plt.plot(history.history['val_loss'], label="Test loss")
  plt.xlabel('epochs')
  plt.ylabel('loss')
  plt.legend(loc='best')
  plt.show()
  model.save('/content/drive/MyDrive/Colab Notebooks/MLAssignment6/model')
  history = load_model('/content/drive/MyDrive/Colab Notebooks/MLAssignment6/model')

  # evaluation = model.evaluate(x_test, y_test, verbose=1)
  # print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))

  
  
q2()

